{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "using Toms566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Develop a Newton's method to approximate $e^{-1}$ that does not involve division. Start at $x_0 = 0.3$ and run this thing.\n",
    "\n",
    "## Solution: \n",
    "Using $f(x) = \\log(\\frac{x}{d})$, we obtain the iterative method: \n",
    "\n",
    "$$x^{k+1} = x^k - (\\log(x^k) - \\log(d)) x^k$$\n",
    "\n",
    "Now, we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see any significant differences between the two $x_0$ runs. One takes longer than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qt: Untested Windows version 10.0 detected!\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newtonRoot (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtonRoot(x0)\n",
    "    # Vars\n",
    "    x = Float64[]\n",
    "    d = e\n",
    "    eps = 1.0e-8\n",
    "    its = 0\n",
    "\n",
    "    # First iteration\n",
    "    push!(x,x0)\n",
    "    push!(x, x[end] - x[end]*(log(x[end]) - log(d)))\n",
    "\n",
    "    # Continued iterations\n",
    "    while abs(x[end] - x[end-1]) > eps\n",
    "        push!(x, x[end] - x[end]*(log(x[end]) - log(d)))\n",
    "        its += 1\n",
    "    end\n",
    "\n",
    "    @printf \"The approximation: %10f\\n\" x[end]\n",
    "    @printf \"The error: %10f\\n\" abs(x[end] - e)\n",
    "    @printf \"Iterations: %5d\\n\" its\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximation:   2.718282\n",
      "The error:   0.000000\n",
      "Iterations:     6\n"
     ]
    }
   ],
   "source": [
    "newtonRoot(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximation:   2.718282\n",
      "The error:   0.000000\n",
      "Iterations:     5\n"
     ]
    }
   ],
   "source": [
    "newtonRoot(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "a) Newton's method on $f(x) = x^q$. What's the convergence ratio there?\n",
    "\n",
    "b) Apply Newton's method to minimize $f(x) = \\| x \\|_2^\\beta$. For what $x_0$ and $\\beta$ does it converge? What happens when $\\beta \\leq 1$?\n",
    "\n",
    "c) Repeat part b, but with Armijo linesearch. \n",
    "\n",
    "## Solution\n",
    "a) If we apply the straight root-finding method, then we have this iterative method:\n",
    "\n",
    "$$x^{k+1} = x^k - \\frac{f(x)}{f'(x)} = x^k - \\frac{(x^k)^q}{q(x^k)^{q-1}}$$\n",
    "\n",
    "$$= x^k - \\frac{1}{q}x^k = x^k (1 - \\frac{1}{q})$$\n",
    "\n",
    "Therefore the method is Q-linear, with $\\mu = 1 - \\frac{1}{q}$. \n",
    "\n",
    "If we apply the method instead to the derivative, then through the same procedure we obtain $\\mu = 1 - \\frac{1}{q-1}$.\n",
    "\n",
    "\n",
    "b) The method is implemented in the next code box. The following values of $\\beta$ don't converge: 2,4. It's not entirely clear to me which x-values converged and which didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtonMethod (generic function with 1 method)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtonMethod(obj, x0; linesearch=true, maxIts=500, optTol=1.0e-8, verbose=false)\n",
    "    its = 0\n",
    "    x = x0\n",
    "    fvals = []\n",
    "    gnormvals = []\n",
    "    (f,g,H) = obj(x)\n",
    "    g0 = g\n",
    "\n",
    "    while (its < maxIts && norm(g,2) > optTol && norm(g,2) > (1.0e-4*norm(g0,2)))\n",
    "        (f,g,H) = obj(x)\n",
    "        \n",
    "        # Modify Hessian if not positive definite\n",
    "        E = eigfact(H);\n",
    "        V = real(E[:vectors]);\n",
    "        lambda = real(diagm(max(E[:values],1e-2)));\n",
    "        d = -V*inv(lambda)*transpose(V)*g;\n",
    "        \n",
    "        # Backtracking linesearch\n",
    "        alpha = 1;\n",
    "        if linesearch\n",
    "            mu = 10^-2.0;\n",
    "            (newf,newg,newH) = obj(x+alpha*d);\n",
    "            while newf > f + (alpha*mu)*(dot(g,d))\n",
    "                (newf, newg, newH) = obj(x + alpha*d);\n",
    "                alpha = alpha/2;\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        x = x + alpha * d\n",
    "        \n",
    "        its += 1\n",
    "        fvals = [fvals; f]\n",
    "        gnormvals = [gnormvals; norm(g,2)]\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        print(\"Done!\\n\")\n",
    "        @printf \"Optimal value: %f\\n\" f\n",
    "        print(\"Location: \\n\")\n",
    "        print(x)\n",
    "        print(\"\\n\")\n",
    "        @printf \"Iterations: %d\\n\" its\n",
    "        print(\"\\n\\n\")\n",
    "    end\n",
    "    \n",
    "    return (x,f,norm(g,2),its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: `isless` has no method matching isless(::Float64, ::Array{Complex{Float64},1})\nClosest candidates are:\n  isless(::Float64, !Matched::Float64)\n  isless(::AbstractFloat, !Matched::AbstractFloat)\n  isless(::Real, !Matched::AbstractFloat)\n  ...\nwhile loading In[26], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: `isless` has no method matching isless(::Float64, ::Array{Complex{Float64},1})\nClosest candidates are:\n  isless(::Float64, !Matched::Float64)\n  isless(::AbstractFloat, !Matched::AbstractFloat)\n  isless(::Real, !Matched::AbstractFloat)\n  ...\nwhile loading In[26], in expression starting on line 2",
      "",
      " in max at operators.jl:57",
      " in newtonMethod at In[25]:15",
      " [inlined code] from In[26]:13",
      " in anonymous at no file:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 | Beta: 2.00 | Starting norm: 7.9014 | x-norm: 4897401552310205102600083262597193231499264.0000\n",
      "\n",
      "Iteration:  2 | Beta: 2.00 | Starting norm: 16.7299 | x-norm: 66805371768545066476874187043981896403910656.0000\n",
      "\n",
      "Iteration:  3 | Beta: 2.00 | Starting norm: 14.3742 | x-norm: 29071007305920772919949555188787904138706944.0000\n",
      "\n",
      "Iteration:  4 | Beta: 2.00 | Starting norm: 20.9539 | x-norm: 71099454311449104169179134318862898661687296.0000\n",
      "\n",
      "Iteration:  5 | Beta: 2.00 | Starting norm: 17.0967 | x-norm: 64777667636550562627748753782536057787514880.0000\n",
      "\n",
      "Iteration:  6 | Beta: 2.00 | Starting norm: 31.6719 | x-norm: 112065468483329341325627254031276325442945024.0000\n",
      "\n",
      "Iteration:  7 | Beta: 2.00 | Starting norm: 22.2877 | x-norm: 108206129052693285721200447297749332879474688.0000\n",
      "\n",
      "Iteration:  8 | Beta: 2.00 | Starting norm: 16.1134 | x-norm: 35965692591921949595633844334669096625897472.0000\n",
      "\n",
      "Iteration:  9 | Beta: 2.00 | Starting norm: 21.2944 | x-norm: 104368148589495952778590222260076624882958336.0000\n",
      "\n",
      "Iteration: 10 | Beta: 2.00 | Starting norm: 22.0236 | x-norm: 105339388959424786319090335361275382112190464.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop to test different \\beta values\n",
    "for b = 2:8\n",
    "    function obj(x)\n",
    "        f = norm(x,2)^b\n",
    "        g = x*b*norm(x,2)^(b-2)\n",
    "        H = b*((b-2)*norm(x,2)^(b-4)*x*x' + norm(x,2)^(b-2)*ones(size(x))*ones(size(x))')\n",
    "        return (f,g,H)\n",
    "    end\n",
    "    \n",
    "    # Loop to try different x_0 points\n",
    "    for j = 1:10\n",
    "        x0 = 10*randn(5,);\n",
    "        (x,f,normg,its) = newtonMethod(obj,x0,linesearch=false,maxIts=10);\n",
    "        @printf \"Iteration: %2d | Beta: %2.2f | Starting norm: %4.4f | x-norm: %4.4f\\n\\n\" j b norm(x0,2) f\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The Armijo linesearch is implemented in the following code box. I randomly sampled points in a radius-10 ball from the origin and tried all values of $\\beta$ from 2 to 8. They all reduced the norm of the point! This is a significant improvement from part b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 | Beta: 2.00 | Starting norm: 17.9916 | x-norm: 9.3288\n",
      "\n",
      "Iteration:  2 | Beta: 2.00 | Starting norm: 30.1060 | x-norm: 10.3129\n",
      "\n",
      "Iteration:  3 | Beta: 2.00 | Starting norm: 15.5384 | x-norm: 9.5009\n",
      "\n",
      "Iteration:  4 | Beta: 2.00 | Starting norm: 21.6784 | x-norm: 4.8251\n",
      "\n",
      "Iteration:  5 | Beta: 2.00 | Starting norm: 23.5745 | x-norm: 10.0854\n",
      "\n",
      "Iteration:  6 | Beta: 2.00 | Starting norm: 15.1349 | x-norm: 4.4240\n",
      "\n",
      "Iteration:  7 | Beta: 2.00 | Starting norm: 14.6318 | x-norm: 3.7839\n",
      "\n",
      "Iteration:  8 | Beta: 2.00 | Starting norm: 23.9553 | x-norm: 6.5890\n",
      "\n",
      "Iteration:  9 | Beta: 2.00 | Starting norm: 14.3567 | x-norm: 1.8709\n",
      "\n",
      "Iteration: 10 | Beta: 2.00 | Starting norm: 20.2260 | x-norm: 0.0770\n",
      "\n",
      "Iteration:  1 | Beta: 3.00 | Starting norm: 13.0066 | x-norm: 4.3410\n",
      "\n",
      "Iteration:  2 | Beta: 3.00 | Starting norm: 8.2552 | x-norm: 2.3606\n",
      "\n",
      "Iteration:  3 | Beta: 3.00 | Starting norm: 16.7944 | x-norm: 7.1489\n",
      "\n",
      "Iteration:  4 | Beta: 3.00 | Starting norm: 15.8737 | x-norm: 3.3764\n",
      "\n",
      "Iteration:  5 | Beta: 3.00 | Starting norm: 13.9509 | x-norm: 1.6597\n",
      "\n",
      "Iteration:  6 | Beta: 3.00 | Starting norm: 22.9116 | x-norm: 3.2897\n",
      "\n",
      "Iteration:  7 | Beta: 3.00 | Starting norm: 12.7303 | x-norm: 3.0934\n",
      "\n",
      "Iteration:  8 | Beta: 3.00 | Starting norm: 34.2905 | x-norm: 15.0490\n",
      "\n",
      "Iteration:  9 | Beta: 3.00 | Starting norm: 35.2234 | x-norm: 27.2615\n",
      "\n",
      "Iteration: 10 | Beta: 3.00 | Starting norm: 16.2573 | x-norm: 6.8507\n",
      "\n",
      "Iteration:  1 | Beta: 4.00 | Starting norm: 11.8376 | x-norm: 8.9000\n",
      "\n",
      "Iteration:  2 | Beta: 4.00 | Starting norm: 21.6800 | x-norm: 2.4128\n",
      "\n",
      "Iteration:  3 | Beta: 4.00 | Starting norm: 13.6611 | x-norm: 12.6289\n",
      "\n",
      "Iteration:  4 | Beta: 4.00 | Starting norm: 22.7051 | x-norm: 6.3695\n",
      "\n",
      "Iteration:  5 | Beta: 4.00 | Starting norm: 11.9563 | x-norm: 9.8332\n",
      "\n",
      "Iteration:  6 | Beta: 4.00 | Starting norm: 17.4518 | x-norm: 4.4545\n",
      "\n",
      "Iteration:  7 | Beta: 4.00 | Starting norm: 10.7636 | x-norm: 5.8555\n",
      "\n",
      "Iteration:  8 | Beta: 4.00 | Starting norm: 27.4528 | x-norm: 7.7094\n",
      "\n",
      "Iteration:  9 | Beta: 4.00 | Starting norm: 23.4838 | x-norm: 12.0806\n",
      "\n",
      "Iteration: 10 | Beta: 4.00 | Starting norm: 35.2938 | x-norm: 16.5396\n",
      "\n",
      "Iteration:  1 | Beta: 5.00 | Starting norm: 16.8734 | x-norm: 12.2554\n",
      "\n",
      "Iteration:  2 | Beta: 5.00 | Starting norm: 25.5623 | x-norm: 0.0359\n",
      "\n",
      "Iteration:  3 | Beta: 5.00 | Starting norm: 25.4935 | x-norm: 4.1459\n",
      "\n",
      "Iteration:  4 | Beta: 5.00 | Starting norm: 23.2876 | x-norm: 5.3862\n",
      "\n",
      "Iteration:  5 | Beta: 5.00 | Starting norm: 19.3138 | x-norm: 0.4067\n",
      "\n",
      "Iteration:  6 | Beta: 5.00 | Starting norm: 14.6662 | x-norm: 5.3758\n",
      "\n",
      "Iteration:  7 | Beta: 5.00 | Starting norm: 22.8985 | x-norm: 7.9229\n",
      "\n",
      "Iteration:  8 | Beta: 5.00 | Starting norm: 15.8850 | x-norm: 10.5141\n",
      "\n",
      "Iteration:  9 | Beta: 5.00 | Starting norm: 20.3381 | x-norm: 1.0900\n",
      "\n",
      "Iteration: 10 | Beta: 5.00 | Starting norm: 23.4786 | x-norm: 7.2034\n",
      "\n",
      "Iteration:  1 | Beta: 6.00 | Starting norm: 21.9141 | x-norm: 8.2991\n",
      "\n",
      "Iteration:  2 | Beta: 6.00 | Starting norm: 33.5143 | x-norm: 15.5244\n",
      "\n",
      "Iteration:  3 | Beta: 6.00 | Starting norm: 12.3629 | x-norm: 8.7374\n",
      "\n",
      "Iteration:  4 | Beta: 6.00 | Starting norm: 14.4717 | x-norm: 8.0545\n",
      "\n",
      "Iteration:  5 | Beta: 6.00 | Starting norm: 14.5923 | x-norm: 3.7583\n",
      "\n",
      "Iteration:  6 | Beta: 6.00 | Starting norm: 22.9498 | x-norm: 1.7246\n",
      "\n",
      "Iteration:  7 | Beta: 6.00 | Starting norm: 17.1283 | x-norm: 0.0562\n",
      "\n",
      "Iteration:  8 | Beta: 6.00 | Starting norm: 12.7325 | x-norm: 1.6916\n",
      "\n",
      "Iteration:  9 | Beta: 6.00 | Starting norm: 9.3837 | x-norm: 3.3194\n",
      "\n",
      "Iteration: 10 | Beta: 6.00 | Starting norm: 11.8333 | x-norm: 9.8311\n",
      "\n",
      "Iteration:  1 | Beta: 7.00 | Starting norm: 26.0205 | x-norm: 2.0302\n",
      "\n",
      "Iteration:  2 | Beta: 7.00 | Starting norm: 16.6017 | x-norm: 3.6512\n",
      "\n",
      "Iteration:  3 | Beta: 7.00 | Starting norm: 19.1915 | x-norm: 2.6140\n",
      "\n",
      "Iteration:  4 | Beta: 7.00 | Starting norm: 10.0262 | x-norm: 7.4650\n",
      "\n",
      "Iteration:  5 | Beta: 7.00 | Starting norm: 18.4824 | x-norm: 13.0713\n",
      "\n",
      "Iteration:  6 | Beta: 7.00 | Starting norm: 16.0222 | x-norm: 11.2258\n",
      "\n",
      "Iteration:  7 | Beta: 7.00 | Starting norm: 6.7817 | x-norm: 0.8409\n",
      "\n",
      "Iteration:  8 | Beta: 7.00 | Starting norm: 19.8310 | x-norm: 5.2198\n",
      "\n",
      "Iteration:  9 | Beta: 7.00 | Starting norm: 34.8834 | x-norm: 14.6066\n",
      "\n",
      "Iteration: 10 | Beta: 7.00 | Starting norm: 21.2107 | x-norm: 9.4767\n",
      "\n",
      "Iteration:  1 | Beta: 8.00 | Starting norm: 31.5867 | x-norm: 0.8274\n",
      "\n",
      "Iteration:  2 | Beta: 8.00 | Starting norm: 12.8152 | x-norm: 7.9298\n",
      "\n",
      "Iteration:  3 | Beta: 8.00 | Starting norm: 13.5136 | x-norm: 1.6032\n",
      "\n",
      "Iteration:  4 | Beta: 8.00 | Starting norm: 13.3257 | x-norm: 1.2432\n",
      "\n",
      "Iteration:  5 | Beta: 8.00 | Starting norm: 17.2083 | x-norm: 5.2984\n",
      "\n",
      "Iteration:  6 | Beta: 8.00 | Starting norm: 18.0586 | x-norm: 8.4451\n",
      "\n",
      "Iteration:  7 | Beta: 8.00 | Starting norm: 25.8932 | x-norm: 6.6557\n",
      "\n",
      "Iteration:  8 | Beta: 8.00 | Starting norm: 41.8070 | x-norm: 38.0454\n",
      "\n",
      "Iteration:  9 | Beta: 8.00 | Starting norm: 25.4488 | x-norm: 7.4345\n",
      "\n",
      "Iteration: 10 | Beta: 8.00 | Starting norm: 24.3313 | x-norm: 9.0535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop to test different \\beta values\n",
    "for b = 2:8\n",
    "    # Loop to try different x_0 points\n",
    "    for j = 1:10\n",
    "            x0 = 10*randn(5,);\n",
    "\n",
    "            function obj(x)\n",
    "                f = norm(x,2)^b\n",
    "                g = x*b*norm(x,2)^(b-2)\n",
    "                H = b*((b-2)*norm(x,2)^(b-4)*x*x' + norm(x,2)^(b-2)*ones(size(x))*ones(size(x))')\n",
    "                return (f,g,H)\n",
    "            end\n",
    "\n",
    "        (x,f,normg,its) = newtonMethod(obj,x0,linesearch=true)\n",
    "\n",
    "            # Iterate\n",
    "            x = x + alpha * d;\n",
    "        end\n",
    "        \n",
    "        @printf \"Iteration: %2d | Beta: %2.2f | Starting norm: %4.4f | x-norm: %4.4f\\n\\n\" j b norm(x0,2) norm(x,2)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Suppose $D^0$ is symmetric and that $D^k$ is updated according to the formula \n",
    "\n",
    "$$D^{k+1} = D^k + \\frac{y^k y^{k'}}{q^{k'} y^{k}}$$\n",
    "\n",
    "where $y^k = p^k - D^k q^k$. Show that we have\n",
    "\n",
    "$$D^{k+1} q^i = p^i$$ \n",
    "\n",
    "for all $k$ and $i \\leq k$. \n",
    "\n",
    "Conclude that for a positive definite quadratic problem, after $n$ steps for which $n$ linearly independent increments $q^0, ..., q^{n-1}$ are obtained, $D^n$ is equal to the inverse Hessian of the cost function.\n",
    "\n",
    "## Solution\n",
    "Don't have this one yet. An update is coming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Add BFGS quasi-Newton option to the previous Newton method and compare it with the regular Newton on Toms566 problems.\n",
    "\n",
    "## Solution\n",
    "The code is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfgsMethod (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bfgsMethod(obj, x0; maxIts = 500, optTol = 1.0e-8, verbose=false)\n",
    "    # Initialize parameters.\n",
    "    its = 0\n",
    "    x = x0\n",
    "    fvals = []\n",
    "    gnormvals = []\n",
    "    (f,g,_) = obj(x)\n",
    "    g0 = g\n",
    "    B = eye(length(g))\n",
    "    \n",
    "    while (its < maxIts && norm(g,2) > optTol && norm(g,2) > (1.0e-4*norm(g0,2)))\n",
    "        (f,g,_) = obj(x)\n",
    "        \n",
    "        # Modify B if not positive definite.\n",
    "        #E = eigfact(B);\n",
    "        #V = E[:vectors];\n",
    "        #lambda = diagm(max(E[:values],1e-2));\n",
    "        #d = -V*inv(lambda)*transpose(V)*g;\n",
    "        \n",
    "        d = B \\ (-g)\n",
    "        \n",
    "        # Backtracking linesearch.\n",
    "        alpha = 1.0;\n",
    "        mu = 10^-1.0;\n",
    "        (newf,newg,_) = obj(x+alpha*d);\n",
    "        while newf > f + (alpha*mu)*(dot(g,d))\n",
    "            (newf,newg,_) = obj(x + alpha*d);\n",
    "            alpha = alpha/2;\n",
    "        end\n",
    "        \n",
    "        # Update x.\n",
    "        x = x + alpha * d\n",
    "        \n",
    "        # Perform the BFGS update.\n",
    "        s = alpha*d\n",
    "        y = newg - g\n",
    "        B = B + (y*y') / dot(y,s) - ((B*s)*(s'*B)) / dot(s,B*s)\n",
    "        \n",
    "        # Increment and store data.\n",
    "        its += 1\n",
    "        fvals = [fvals; f]\n",
    "        gnormvals = [gnormvals; norm(g,2)]\n",
    "    end\n",
    "    \n",
    "    # Maybe pretty print things.\n",
    "    if verbose\n",
    "        print(\"Done!\\n\")\n",
    "        @printf \"Optimal value: %f\\n\" f\n",
    "        print(\"Location: \\n\")\n",
    "        print(x)\n",
    "        print(\"\\n\")\n",
    "        @printf \"Iterations: %d\\n\" its\n",
    "        print(\"\\n\\n\")\n",
    "    end\n",
    "    \n",
    "    return (x,f,norm(g,2),its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Dmitry's Newton method on all problems!\n",
      "\n",
      "    i                   Problem name                 f(x)         |grad(f(x))|                  its \n",
      "    1                Hellical valley             0.000010             0.110821                   35 \n",
      "    2                    Bigg's EXP6             0.255601             0.138184                  500 \n",
      "    3                       Gaussian             0.000000             0.000000                    3 \n",
      "    4                         Powell             0.000101             1.999961                   28 \n",
      "    5                      Box 3-dim             0.000165             0.001989                   19 \n",
      "    6           Variably dimensioned         31543.569717       1392845.689824                    4 \n",
      "    7                         Watson             3.527223             5.749285                  500 \n",
      "    8                      Penalty I          6396.051688          2865.310581                    9 \n",
      "    9                     Penalty II            89.602610             9.705922                   13 \n",
      "   10             Brown badly scaled             0.000000             3.352820                    8 \n",
      "   11                Brown and Denis       1620843.806890        445243.061723                  500 \n",
      "   12  Gulf research and development             0.010001             0.001543                    9 \n",
      "   13                  Trigonometric             0.000004             0.000139                  500 \n",
      "   14            Extended rosenbrock           428.729348           961.480139                  500 \n",
      "   15       Extended Powell singular          2153.325921          1331.869213                  500 \n",
      "   16                          Beale             0.000000             0.002172                   10 \n",
      "   17                           Wood             7.876516             0.150289                    8 \n",
      "  "
     ]
    }
   ],
   "source": [
    "print(\"Running Dmitry's Newton method on all problems!\\n\\n\")\n",
    "@printf \"%5s %30s %20s %20s %20s \\n\" \"i\" \"Problem name\" \"f(x)\" \"|grad(f(x))|\" \"its\"\n",
    "\n",
    "for i = 1:18\n",
    "    p = Problem(i)\n",
    "    \n",
    "    function obj(x)\n",
    "        return (p.obj(x), p.grd(x), p.hes(x))\n",
    "    end\n",
    "    \n",
    "    (x,f,gnorm,its) = newtonMethod(obj,p.x0)\n",
    "    @printf \"%5d %30s %20f %20f %20d \\n\" i p.name f gnorm its\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18                      Chebyquad             0.008949             1.018371                  500 \n",
      "Running Dmitry's BFGS method on all problems!\n",
      "\n",
      "    i                   Problem name                 f(x)         |grad(f(x))|                  its \n",
      "    1                Hellical valley             0.000004             0.046511                   24 \n",
      "    2                    Bigg's EXP6             0.005656             0.000124                   36 \n",
      "    3                       Gaussian             0.000000             0.000000                    6 \n",
      "    4                         Powell             0.000000             1.066644                   59 \n",
      "    5                      Box 3-dim             0.020431             0.010411                   17 \n",
      "    6           Variably dimensioned        425274.040251       9907956.334987                    8 \n",
      "    7                         Watson             0.000131             0.005266                   29 \n",
      "    8                      Penalty I           140.495195           164.944349                    9 \n",
      "    9                     Penalty II            88.360321            16.559610                   73 \n",
      "   10             Brown badly scaled             0.000000            25.161030                   12 \n",
      "   11                Brown and Denis         85824.779242           132.958948                   17 \n",
      "   12  Gulf research and development                  NaN                  NaN                   13 \n",
      "   13                  Trigonometric             0.000004             0.000004                   42 \n",
      "   14            Extended rosenbrock             0.001252             0.070794                  151 \n",
      "   15       Extended Powell singular             0.002348             0.173321                   31 \n",
      "   16                          Beale             0.000003             0.002182                   11 \n",
      "   17                           Wood             3.877164             1.624107                   18 \n",
      "   18                      Chebyquad             0.005386             0.000195                  131 \n"
     ]
    }
   ],
   "source": [
    "print(\"Running Dmitry's BFGS method on all problems!\\n\\n\")\n",
    "@printf \"%5s %30s %20s %20s %20s \\n\" \"i\" \"Problem name\" \"f(x)\" \"|grad(f(x))|\" \"its\"\n",
    "\n",
    "for i = 1:18\n",
    "    p = Problem(i)\n",
    "    \n",
    "    function obj(x)\n",
    "        return (p.obj(x), p.grd(x), p.hes(x))\n",
    "    end\n",
    "    \n",
    "    (x,f,gnorm,its) = bfgsMethod(obj,p.x0)\n",
    "    @printf \"%5d %30s %20f %20f %20d \\n\" i p.name f gnorm its\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
