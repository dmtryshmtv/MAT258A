{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qt: Untested Windows version 10.0 detected!\r\n"
     ]
    }
   ],
   "source": [
    "using PyPlot\n",
    "using Toms566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Develop a Newton's method to approximate $e^{-1}$ that does not involve division. Start at $x_0 = 0.3$ and run this thing.\n",
    "\n",
    "## Solution: \n",
    "Using $f(x) = \\log(\\frac{x}{d})$, we obtain the iterative method: \n",
    "\n",
    "$$x^{k+1} = x^k - (\\log(x^k) - \\log(d)) x^k$$\n",
    "\n",
    "Now, we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see any significant differences between the two $x_0$ runs. One takes longer than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qt: Untested Windows version 10.0 detected!\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newtonRoot (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtonRoot(x0)\n",
    "    # Vars\n",
    "    x = Float64[]\n",
    "    d = e\n",
    "    eps = 1.0e-8\n",
    "    its = 0\n",
    "\n",
    "    # First iteration\n",
    "    push!(x,x0)\n",
    "    push!(x, x[end] - x[end]*(log(x[end]) - log(d)))\n",
    "\n",
    "    # Continued iterations\n",
    "    while abs(x[end] - x[end-1]) > eps\n",
    "        push!(x, x[end] - x[end]*(log(x[end]) - log(d)))\n",
    "        its += 1\n",
    "    end\n",
    "\n",
    "    @printf \"The approximation: %10f\\n\" x[end]\n",
    "    @printf \"The error: %10f\\n\" abs(x[end] - e)\n",
    "    @printf \"Iterations: %5d\\n\" its\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximation:   2.718282\n",
      "The error:   0.000000\n",
      "Iterations:     6\n"
     ]
    }
   ],
   "source": [
    "newtonRoot(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximation:   2.718282\n",
      "The error:   0.000000\n",
      "Iterations:     5\n"
     ]
    }
   ],
   "source": [
    "newtonRoot(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "a) Newton's method on $f(x) = x^q$. What's the convergence ratio there?\n",
    "\n",
    "b) Apply Newton's method to minimize $f(x) = \\| x \\|_2^\\beta$. For what $x_0$ and $\\beta$ does it converge? What happens when $\\beta \\leq 1$?\n",
    "\n",
    "c) Repeat part b, but with Armijo linesearch. \n",
    "\n",
    "## Solution\n",
    "a) If we apply the straight root-finding method, then we have this iterative method:\n",
    "\n",
    "$$x^{k+1} = x^k - \\frac{f(x)}{f'(x)} = x^k - \\frac{(x^k)^q}{q(x^k)^{q-1}}$$\n",
    "\n",
    "$$= x^k - \\frac{1}{q}x^k = x^k (1 - \\frac{1}{q})$$\n",
    "\n",
    "Therefore the method is Q-linear, with $\\mu = 1 - \\frac{1}{q}$. \n",
    "\n",
    "If we apply the method instead to the derivative, then through the same procedure we obtain $\\mu = 1 - \\frac{1}{q-1}$.\n",
    "\n",
    "\n",
    "b) The method is implemented in the next code box. Working out the theory, the gradient of $f(x) = \\|x\\|_2^\\beta$ is:\n",
    "\n",
    "$$\\nabla f(x) = \\beta \\|x\\|_2^{\\beta-2} x$$\n",
    "\n",
    "and the Hessian is:\n",
    "\n",
    "$$H = \\beta( (\\beta - 2) \\|x\\|_2^{\\beta - 4} x^T x + \\|x\\|_2^{\\beta -2} I$$\n",
    "\n",
    "Note that the gradient is colinear to $x$. Conveniently, $x$ is an eigenvector of the Hessian, so we can easily compute the action of the Hessian on the gradient. The eigenvalue for $x$ is:\n",
    "\n",
    "$$Hx = \\|x\\|^{\\beta - 2} \\beta(\\beta - 1)x$$\n",
    "\n",
    "Inverting it and applying it to $f$, we obtain:\n",
    "\n",
    "$$H^{-1}\\nabla f(x) = H^{-1} \\beta \\|x\\|^{\\beta -2} x = \\frac{1}{\\beta - 1}x$$\n",
    "\n",
    "Thus, it works the same as the single-variate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtonMethod (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtonMethod(obj, x0; linesearch=true, maxIts=500, optTol=1.0e-8, verbose=false)\n",
    "    its = 0\n",
    "    x = x0\n",
    "    fvals = []\n",
    "    gnormvals = []\n",
    "    (f,g,H) = obj(x)\n",
    "    g0 = g\n",
    "\n",
    "    while (its < maxIts && norm(g,2) > optTol && norm(g,2) > (1.0e-4*norm(g0,2)))\n",
    "        (f,g,H) = obj(x)\n",
    "        \n",
    "        # Modify Hessian if not positive definite\n",
    "        E = eigfact(H);\n",
    "        V = real(E[:vectors]);\n",
    "        lambda = diagm(max(real(E[:values]),1e-2));\n",
    "        d = -V*inv(real(lambda))*transpose(V)*g;\n",
    "        \n",
    "        # Backtracking linesearch\n",
    "        alpha = 0.01;\n",
    "        if linesearch\n",
    "            mu = 10^-2.0;\n",
    "            (newf,newg,newH) = obj(x+alpha*d);\n",
    "            while newf > f + (alpha*mu)*(dot(g,d))\n",
    "                (newf, newg, newH) = obj(x + alpha*d);\n",
    "                alpha = alpha / 2.0;                \n",
    "            end\n",
    "        end\n",
    "        \n",
    "        x = x + alpha * d\n",
    "        \n",
    "        its += 1\n",
    "        fvals = [fvals; f]\n",
    "        gnormvals = [gnormvals; norm(g,2)]\n",
    "    end\n",
    "    \n",
    "    if verbose\n",
    "        print(\"Done!\\n\")\n",
    "        @printf \"Optimal value: %f\\n\" f\n",
    "        print(\"Location: \\n\")\n",
    "        print(x)\n",
    "        print(\"\\n\")\n",
    "        @printf \"Iterations: %d\\n\" its\n",
    "        print(\"\\n\\n\")\n",
    "    end\n",
    "    \n",
    "    return (x,f,norm(g,2),its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 | Beta: 2.00 | Starting norm: 22.3414 | x-norm: 416.5391\n",
      "\n",
      "Iteration:  2 | Beta: 2.00 | Starting norm: 27.4397 | x-norm: 628.3343\n",
      "\n",
      "Iteration:  3 | Beta: 2.00 | Starting norm: 21.4835 | x-norm: 385.1612\n",
      "\n",
      "Iteration:  4 | Beta: 2.00 | Starting norm: 27.0778 | x-norm: 611.8699\n",
      "\n",
      "Iteration:  5 | Beta: 2.00 | Starting norm: 27.3511 | x-norm: 624.2847\n",
      "\n",
      "Iteration:  6 | Beta: 2.00 | Starting norm: 33.4374 | x-norm: 933.0341\n",
      "\n",
      "Iteration:  7 | Beta: 2.00 | Starting norm: 40.7096 | x-norm: 1383.0152\n",
      "\n",
      "Iteration:  8 | Beta: 2.00 | Starting norm: 16.5057 | x-norm: 227.3537\n",
      "\n",
      "Iteration:  9 | Beta: 2.00 | Starting norm: 16.9517 | x-norm: 239.8056\n",
      "\n",
      "Iteration: 10 | Beta: 2.00 | Starting norm: 15.9210 | x-norm: 211.5320\n",
      "\n",
      "Iteration:  1 | Beta: 3.00 | Starting norm: 11.7548 | x-norm: 1418.6408\n",
      "\n",
      "Iteration:  2 | Beta: 3.00 | Starting norm: 27.8451 | x-norm: 18856.9179\n",
      "\n",
      "Iteration:  3 | Beta: 3.00 | Starting norm: 23.1386 | x-norm: 10820.1235\n",
      "\n",
      "Iteration:  4 | Beta: 3.00 | Starting norm: 23.8521 | x-norm: 11852.2791\n",
      "\n",
      "Iteration:  5 | Beta: 3.00 | Starting norm: 28.5186 | x-norm: 20258.5929\n",
      "\n",
      "Iteration:  6 | Beta: 3.00 | Starting norm: 18.6810 | x-norm: 5694.0506\n",
      "\n",
      "Iteration:  7 | Beta: 3.00 | Starting norm: 50.1023 | x-norm: 109848.9933\n",
      "\n",
      "Iteration:  8 | Beta: 3.00 | Starting norm: 12.4534 | x-norm: 1686.8978\n",
      "\n",
      "Iteration:  9 | Beta: 3.00 | Starting norm: 21.8281 | x-norm: 9083.8842\n",
      "\n",
      "Iteration: 10 | Beta: 3.00 | Starting norm: 15.6905 | x-norm: 3373.9040\n",
      "\n",
      "Iteration:  1 | Beta: 4.00 | Starting norm: 9.9036 | x-norm: 8530.4646\n",
      "\n",
      "Iteration:  2 | Beta: 4.00 | Starting norm: 16.7199 | x-norm: 69300.6596\n",
      "\n",
      "Iteration:  3 | Beta: 4.00 | Starting norm: 26.1178 | x-norm: 412615.4232\n",
      "\n",
      "Iteration:  4 | Beta: 4.00 | Starting norm: 19.2634 | x-norm: 122102.9881\n",
      "\n",
      "Iteration:  5 | Beta: 4.00 | Starting norm: 19.1615 | x-norm: 119540.6803\n",
      "\n",
      "Iteration:  6 | Beta: 4.00 | Starting norm: 22.7538 | x-norm: 237690.3471\n",
      "\n",
      "Iteration:  7 | Beta: 4.00 | Starting norm: 23.1058 | x-norm: 252745.3991\n",
      "\n",
      "Iteration:  8 | Beta: 4.00 | Starting norm: 18.4882 | x-norm: 103604.9897\n",
      "\n",
      "Iteration:  9 | Beta: 4.00 | Starting norm: 39.7011 | x-norm: 2202968.8337\n",
      "\n",
      "Iteration: 10 | Beta: 4.00 | Starting norm: 16.9245 | x-norm: 72755.2460\n",
      "\n",
      "Iteration:  1 | Beta: 5.00 | Starting norm: 23.5178 | x-norm: 6427763.5337\n",
      "\n",
      "Iteration:  2 | Beta: 5.00 | Starting norm: 22.7518 | x-norm: 5446990.3405\n",
      "\n",
      "Iteration:  3 | Beta: 5.00 | Starting norm: 28.0110 | x-norm: 15407329.6446\n",
      "\n",
      "Iteration:  4 | Beta: 5.00 | Starting norm: 10.6681 | x-norm: 123457.4776\n",
      "\n",
      "Iteration:  5 | Beta: 5.00 | Starting norm: 17.7264 | x-norm: 1563829.3354\n",
      "\n",
      "Iteration:  6 | Beta: 5.00 | Starting norm: 14.0554 | x-norm: 490109.5182\n",
      "\n",
      "Iteration:  7 | Beta: 5.00 | Starting norm: 24.1860 | x-norm: 7394398.4930\n",
      "\n",
      "Iteration:  8 | Beta: 5.00 | Starting norm: 23.3857 | x-norm: 6249259.1977\n",
      "\n",
      "Iteration:  9 | Beta: 5.00 | Starting norm: 13.0198 | x-norm: 334275.6094\n",
      "\n",
      "Iteration: 10 | Beta: 5.00 | Starting norm: 20.9911 | x-norm: 3641300.7188\n",
      "\n",
      "Iteration:  1 | Beta: 6.00 | Starting norm: 19.6711 | x-norm: 52002624.9449\n",
      "\n",
      "Iteration:  2 | Beta: 6.00 | Starting norm: 16.5354 | x-norm: 18345562.0700\n",
      "\n",
      "Iteration:  3 | Beta: 6.00 | Starting norm: 30.5040 | x-norm: 723080023.9511\n",
      "\n",
      "Iteration:  4 | Beta: 6.00 | Starting norm: 18.2937 | x-norm: 33640218.3415\n",
      "\n",
      "Iteration:  5 | Beta: 6.00 | Starting norm: 16.3606 | x-norm: 17212748.5387\n",
      "\n",
      "Iteration:  6 | Beta: 6.00 | Starting norm: 26.7904 | x-norm: 331835123.8143\n",
      "\n",
      "Iteration:  7 | Beta: 6.00 | Starting norm: 16.8417 | x-norm: 20481967.2224\n",
      "\n",
      "Iteration:  8 | Beta: 6.00 | Starting norm: 34.9753 | x-norm: 1642931349.0423\n",
      "\n",
      "Iteration:  9 | Beta: 6.00 | Starting norm: 24.4041 | x-norm: 189592978.0252\n",
      "\n",
      "Iteration: 10 | Beta: 6.00 | Starting norm: 24.9716 | x-norm: 217635106.5456\n",
      "\n",
      "Iteration:  1 | Beta: 7.00 | Starting norm: 17.4234 | x-norm: 438832426.6263\n",
      "\n",
      "Iteration:  2 | Beta: 7.00 | Starting norm: 16.8979 | x-norm: 354145258.8324\n",
      "\n",
      "Iteration:  3 | Beta: 7.00 | Starting norm: 20.7532 | x-norm: 1492634935.1331\n",
      "\n",
      "Iteration:  4 | Beta: 7.00 | Starting norm: 15.2885 | x-norm: 175755148.5120\n",
      "\n",
      "Iteration:  5 | Beta: 7.00 | Starting norm: 29.0739 | x-norm: 15808229190.2415\n",
      "\n",
      "Iteration:  6 | Beta: 7.00 | Starting norm: 22.9644 | x-norm: 3032139228.5395\n",
      "\n",
      "Iteration:  7 | Beta: 7.00 | Starting norm: 21.4638 | x-norm: 1889343758.0362\n",
      "\n",
      "Iteration:  8 | Beta: 7.00 | Starting norm: 28.5572 | x-norm: 13943293534.5764\n",
      "\n",
      "Iteration:  9 | Beta: 7.00 | Starting norm: 13.7027 | x-norm: 81658884.7848\n",
      "\n",
      "Iteration: 10 | Beta: 7.00 | Starting norm: 14.5744 | x-norm: 125744769.7581\n",
      "\n",
      "Iteration:  1 | Beta: 8.00 | Starting norm: 17.5269 | x-norm: 8033959338.4904\n",
      "\n",
      "Iteration:  2 | Beta: 8.00 | Starting norm: 32.4952 | x-norm: 1121641865211.8689\n",
      "\n",
      "Iteration:  3 | Beta: 8.00 | Starting norm: 21.7293 | x-norm: 44838977870.7827\n",
      "\n",
      "Iteration:  4 | Beta: 8.00 | Starting norm: 23.8547 | x-norm: 94600909671.2739\n",
      "\n",
      "Iteration:  5 | Beta: 8.00 | Starting norm: 29.7128 | x-norm: 548085604456.2992\n",
      "\n",
      "Iteration:  6 | Beta: 8.00 | Starting norm: 9.5570 | x-norm: 62784802.0090\n",
      "\n",
      "Iteration:  7 | Beta: 8.00 | Starting norm: 16.6719 | x-norm: 5384858113.1016\n",
      "\n",
      "Iteration:  8 | Beta: 8.00 | Starting norm: 20.6318 | x-norm: 29620466269.8562\n",
      "\n",
      "Iteration:  9 | Beta: 8.00 | Starting norm: 38.8136 | x-norm: 4647004430589.3564\n",
      "\n",
      "Iteration: 10 | Beta: 8.00 | Starting norm: 28.4596 | x-norm: 388268830902.1071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop to test different \\beta values\n",
    "for b = 2:8\n",
    "    function obj(x)\n",
    "        f = norm(x,2)^b\n",
    "        g = x*b*norm(x,2)^(b-2)\n",
    "        H = b*norm(x,2)^(b-2)*((b-2)*norm(x,2)^(-2)*x*x' + eye(length(g)))\n",
    "        return (f,g,H)\n",
    "    end\n",
    "    \n",
    "    # Loop to try different x_0 points\n",
    "    for j = 1:10\n",
    "        x0 = 10.0*randn(5,);\n",
    "        (x,f,normg,its) = newtonMethod(obj,x0,linesearch=false,maxIts=10);\n",
    "        @printf \"Iteration: %2d | Beta: %2.2f | Starting norm: %4.4f | x-norm: %4.4f\\n\\n\" j b norm(x0,2) f\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The Armijo linesearch is implemented in the following code box. I randomly sampled points in a radius-10 ball from the origin and tried all values of $\\beta$ from 2 to 8. They all reduced the norm of the point! This is a significant improvement from part b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1 | Beta: 2.00 | Starting norm: 28.4551 | x-norm: 0.1870\n",
      "\n",
      "Iteration:  2 | Beta: 2.00 | Starting norm: 24.9736 | x-norm: 0.1641\n",
      "\n",
      "Iteration:  3 | Beta: 2.00 | Starting norm: 26.3656 | x-norm: 0.1732\n",
      "\n",
      "Iteration:  4 | Beta: 2.00 | Starting norm: 18.4614 | x-norm: 0.1213\n",
      "\n",
      "Iteration:  5 | Beta: 2.00 | Starting norm: 10.5979 | x-norm: 0.0696\n",
      "\n",
      "Iteration:  6 | Beta: 2.00 | Starting norm: 9.9611 | x-norm: 0.0654\n",
      "\n",
      "Iteration:  7 | Beta: 2.00 | Starting norm: 21.9802 | x-norm: 0.1444\n",
      "\n",
      "Iteration:  8 | Beta: 2.00 | Starting norm: 20.1679 | x-norm: 0.1325\n",
      "\n",
      "Iteration:  9 | Beta: 2.00 | Starting norm: 15.7624 | x-norm: 0.1036\n",
      "\n",
      "Iteration: 10 | Beta: 2.00 | Starting norm: 16.9582 | x-norm: 0.1114\n",
      "\n",
      "Iteration:  1 | Beta: 3.00 | Starting norm: 12.3744 | x-norm: 1.0094\n",
      "\n",
      "Iteration:  2 | Beta: 3.00 | Starting norm: 14.1306 | x-norm: 1.1527\n",
      "\n",
      "Iteration:  3 | Beta: 3.00 | Starting norm: 24.7383 | x-norm: 2.0179\n",
      "\n",
      "Iteration:  4 | Beta: 3.00 | Starting norm: 14.5398 | x-norm: 1.1860\n",
      "\n",
      "Iteration:  5 | Beta: 3.00 | Starting norm: 25.0816 | x-norm: 2.0460\n",
      "\n",
      "Iteration:  6 | Beta: 3.00 | Starting norm: 18.4845 | x-norm: 1.5078\n",
      "\n",
      "Iteration:  7 | Beta: 3.00 | Starting norm: 15.1030 | x-norm: 1.2320\n",
      "\n",
      "Iteration:  8 | Beta: 3.00 | Starting norm: 40.7837 | x-norm: 3.3268\n",
      "\n",
      "Iteration:  9 | Beta: 3.00 | Starting norm: 15.8195 | x-norm: 1.2904\n",
      "\n",
      "Iteration: 10 | Beta: 3.00 | Starting norm: 22.2470 | x-norm: 1.8147\n",
      "\n",
      "Iteration:  1 | Beta: 4.00 | Starting norm: 19.4128 | x-norm: 3.6564\n",
      "\n",
      "Iteration:  2 | Beta: 4.00 | Starting norm: 16.0201 | x-norm: 3.0174\n",
      "\n",
      "Iteration:  3 | Beta: 4.00 | Starting norm: 16.5603 | x-norm: 3.1191\n",
      "\n",
      "Iteration:  4 | Beta: 4.00 | Starting norm: 16.0325 | x-norm: 3.0197\n",
      "\n",
      "Iteration:  5 | Beta: 4.00 | Starting norm: 21.4467 | x-norm: 4.0395\n",
      "\n",
      "Iteration:  6 | Beta: 4.00 | Starting norm: 29.0712 | x-norm: 5.4756\n",
      "\n",
      "Iteration:  7 | Beta: 4.00 | Starting norm: 20.0086 | x-norm: 3.7686\n",
      "\n",
      "Iteration:  8 | Beta: 4.00 | Starting norm: 19.8433 | x-norm: 3.7375\n",
      "\n",
      "Iteration:  9 | Beta: 4.00 | Starting norm: 28.5693 | x-norm: 5.3811\n",
      "\n",
      "Iteration: 10 | Beta: 4.00 | Starting norm: 25.9251 | x-norm: 4.8830\n",
      "\n",
      "Iteration:  1 | Beta: 5.00 | Starting norm: 16.5753 | x-norm: 4.7415\n",
      "\n",
      "Iteration:  2 | Beta: 5.00 | Starting norm: 28.5223 | x-norm: 8.1590\n",
      "\n",
      "Iteration:  3 | Beta: 5.00 | Starting norm: 31.5170 | x-norm: 9.0157\n",
      "\n",
      "Iteration:  4 | Beta: 5.00 | Starting norm: 26.4581 | x-norm: 7.5685\n",
      "\n",
      "Iteration:  5 | Beta: 5.00 | Starting norm: 24.3986 | x-norm: 6.9794\n",
      "\n",
      "Iteration:  6 | Beta: 5.00 | Starting norm: 31.3817 | x-norm: 8.9769\n",
      "\n",
      "Iteration:  7 | Beta: 5.00 | Starting norm: 31.5216 | x-norm: 9.0170\n",
      "\n",
      "Iteration:  8 | Beta: 5.00 | Starting norm: 25.8835 | x-norm: 7.4041\n",
      "\n",
      "Iteration:  9 | Beta: 5.00 | Starting norm: 10.3226 | x-norm: 2.9529\n",
      "\n",
      "Iteration: 10 | Beta: 5.00 | Starting norm: 20.7796 | x-norm: 5.9441\n",
      "\n",
      "Iteration:  1 | Beta: 6.00 | Starting norm: 25.2149 | x-norm: 9.2667\n",
      "\n",
      "Iteration:  2 | Beta: 6.00 | Starting norm: 20.4665 | x-norm: 7.5217\n",
      "\n",
      "Iteration:  3 | Beta: 6.00 | Starting norm: 24.0778 | x-norm: 8.8489\n",
      "\n",
      "Iteration:  4 | Beta: 6.00 | Starting norm: 21.2377 | x-norm: 7.8051\n",
      "\n",
      "Iteration:  5 | Beta: 6.00 | Starting norm: 17.4071 | x-norm: 6.3973\n",
      "\n",
      "Iteration:  6 | Beta: 6.00 | Starting norm: 10.6193 | x-norm: 3.9027\n",
      "\n",
      "Iteration:  7 | Beta: 6.00 | Starting norm: 33.5344 | x-norm: 12.3243\n",
      "\n",
      "Iteration:  8 | Beta: 6.00 | Starting norm: 35.7411 | x-norm: 13.1353\n",
      "\n",
      "Iteration:  9 | Beta: 6.00 | Starting norm: 28.8086 | x-norm: 10.5875\n",
      "\n",
      "Iteration: 10 | Beta: 6.00 | Starting norm: 19.1133 | x-norm: 7.0243\n",
      "\n",
      "Iteration:  1 | Beta: 7.00 | Starting norm: 31.8494 | x-norm: 13.8321\n",
      "\n",
      "Iteration:  2 | Beta: 7.00 | Starting norm: 29.3382 | x-norm: 12.7415\n",
      "\n",
      "Iteration:  3 | Beta: 7.00 | Starting norm: 23.8086 | x-norm: 10.3400\n",
      "\n",
      "Iteration:  4 | Beta: 7.00 | Starting norm: 10.2919 | x-norm: 4.4697\n",
      "\n",
      "Iteration:  5 | Beta: 7.00 | Starting norm: 21.6091 | x-norm: 9.3848\n",
      "\n",
      "Iteration:  6 | Beta: 7.00 | Starting norm: 16.6858 | x-norm: 7.2466\n",
      "\n",
      "Iteration:  7 | Beta: 7.00 | Starting norm: 12.0010 | x-norm: 5.2120\n",
      "\n",
      "Iteration:  8 | Beta: 7.00 | Starting norm: 14.7557 | x-norm: 6.4084\n",
      "\n",
      "Iteration:  9 | Beta: 7.00 | Starting norm: 20.2223 | x-norm: 8.7825\n",
      "\n",
      "Iteration: 10 | Beta: 7.00 | Starting norm: 21.1681 | x-norm: 9.1932\n",
      "\n",
      "Iteration:  1 | Beta: 8.00 | Starting norm: 9.5317 | x-norm: 4.6638\n",
      "\n",
      "Iteration:  2 | Beta: 8.00 | Starting norm: 12.2381 | x-norm: 5.9880\n",
      "\n",
      "Iteration:  3 | Beta: 8.00 | Starting norm: 15.0216 | x-norm: 7.3500\n",
      "\n",
      "Iteration:  4 | Beta: 8.00 | Starting norm: 25.8500 | x-norm: 12.6482\n",
      "\n",
      "Iteration:  5 | Beta: 8.00 | Starting norm: 29.2520 | x-norm: 14.3128\n",
      "\n",
      "Iteration:  6 | Beta: 8.00 | Starting norm: 25.5686 | x-norm: 12.5105\n",
      "\n",
      "Iteration:  7 | Beta: 8.00 | Starting norm: 13.9808 | x-norm: 6.8407\n",
      "\n",
      "Iteration:  8 | Beta: 8.00 | Starting norm: 34.3941 | x-norm: 16.8288\n",
      "\n",
      "Iteration:  9 | Beta: 8.00 | Starting norm: 15.0179 | x-norm: 7.3482\n",
      "\n",
      "Iteration: 10 | Beta: 8.00 | Starting norm: 19.3300 | x-norm: 9.4580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop to test different \\beta values\n",
    "for b = 2:8\n",
    "    function obj(x)\n",
    "        f = norm(x,2)^b\n",
    "        g = x*b*norm(x,2)^(b-2)\n",
    "        H = b*norm(x,2)^(b-2)*((b-2)*norm(x,2)^(-2)*x*x' + eye(length(g)))\n",
    "        return (f,g,H)\n",
    "    end\n",
    "    \n",
    "    # Loop to try different x_0 points\n",
    "    for j = 1:10\n",
    "        x0 = 10*randn(5,);\n",
    "        (x,f,normg,its) = newtonMethod(obj,x0,linesearch=true)        \n",
    "        @printf \"Iteration: %2d | Beta: %2.2f | Starting norm: %4.4f | x-norm: %4.4f\\n\\n\" j b norm(x0,2) norm(x,2)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Suppose $D^0$ is symmetric and that $D^k$ is updated according to the formula \n",
    "\n",
    "$$D^{k+1} = D^k + \\frac{y^k y^{k'}}{q^{k'} y^{k}}$$\n",
    "\n",
    "where $y^k = p^k - D^k q^k$. Show that we have\n",
    "\n",
    "$$D^{k+1} q^i = p^i$$ \n",
    "\n",
    "for all $k$ and $i \\leq k$. \n",
    "\n",
    "Conclude that for a positive definite quadratic problem, after $n$ steps for which $n$ linearly independent increments $q^0, ..., q^{n-1}$ are obtained, $D^n$ is equal to the inverse Hessian of the cost function.\n",
    "\n",
    "## Solution\n",
    "Don't have this one yet. An update is coming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Add BFGS quasi-Newton option to the previous Newton method and compare it with the regular Newton on Toms566 problems.\n",
    "\n",
    "## Solution\n",
    "The code is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfgsMethod (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bfgsMethod(obj, x0; maxIts = 500, optTol = 1.0e-8, verbose=false)\n",
    "    # Initialize parameters.\n",
    "    its = 0\n",
    "    x = x0\n",
    "    fvals = []\n",
    "    gnormvals = []\n",
    "    (f,g,_) = obj(x)\n",
    "    g0 = g\n",
    "    B = eye(length(g))\n",
    "    \n",
    "    while (its < maxIts && norm(g,2) > optTol && norm(g,2) > (1.0e-4*norm(g0,2)))\n",
    "        (f,g,_) = obj(x)\n",
    "        \n",
    "        # Modify B if not positive definite.\n",
    "        #E = eigfact(B);\n",
    "        #V = E[:vectors];\n",
    "        #lambda = diagm(max(E[:values],1e-2));\n",
    "        #d = -V*inv(lambda)*transpose(V)*g;\n",
    "        \n",
    "        d = B \\ (-g)\n",
    "        \n",
    "        # Backtracking linesearch.\n",
    "        alpha = 1.0;\n",
    "        mu = 10^-1.0;\n",
    "        (newf,newg,_) = obj(x+alpha*d);\n",
    "        while newf > f + (alpha*mu)*(dot(g,d))\n",
    "            (newf,newg,_) = obj(x + alpha*d);\n",
    "            alpha = alpha/2;\n",
    "        end\n",
    "        \n",
    "        # Update x.\n",
    "        x = x + alpha * d\n",
    "        \n",
    "        # Perform the BFGS update.\n",
    "        s = alpha*d\n",
    "        y = newg - g\n",
    "        B = B + (y*y') / dot(y,s) - ((B*s)*(s'*B)) / dot(s,B*s)\n",
    "        \n",
    "        # Increment and store data.\n",
    "        its += 1\n",
    "        fvals = [fvals; f]\n",
    "        gnormvals = [gnormvals; norm(g,2)]\n",
    "    end\n",
    "    \n",
    "    # Maybe pretty print things.\n",
    "    if verbose\n",
    "        print(\"Done!\\n\")\n",
    "        @printf \"Optimal value: %f\\n\" f\n",
    "        print(\"Location: \\n\")\n",
    "        print(x)\n",
    "        print(\"\\n\")\n",
    "        @printf \"Iterations: %d\\n\" its\n",
    "        print(\"\\n\\n\")\n",
    "    end\n",
    "    \n",
    "    return (x,f,norm(g,2),its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Dmitry's Newton method on all problems!\n",
      "\n",
      "    i                   Problem name                 f(x)         |grad(f(x))|                  its \n",
      "    1                Hellical valley             0.000010             0.110821                   35 \n",
      "    2                    Bigg's EXP6             0.255601             0.138184                  500 \n",
      "    3                       Gaussian             0.000000             0.000000                    3 \n",
      "    4                         Powell             0.000101             1.999961                   28 \n",
      "    5                      Box 3-dim             0.000165             0.001989                   19 \n",
      "    6           Variably dimensioned         31543.569717       1392845.689824                    4 \n",
      "    7                         Watson             3.527223             5.749285                  500 \n",
      "    8                      Penalty I          6396.051688          2865.310581                    9 \n",
      "    9                     Penalty II            89.602610             9.705922                   13 \n",
      "   10             Brown badly scaled             0.000000             3.352820                    8 \n",
      "   11                Brown and Denis       1620843.806890        445243.061723                  500 \n",
      "   12  Gulf research and development             0.010001             0.001543                    9 \n",
      "   13                  Trigonometric             0.000004             0.000139                  500 \n",
      "   14            Extended rosenbrock           428.729348           961.480139                  500 \n",
      "   15       Extended Powell singular          2153.325921          1331.869213                  500 \n",
      "   16                          Beale             0.000000             0.002172                   10 \n",
      "   17                           Wood             7.876516             0.150289                    8 \n",
      "  "
     ]
    }
   ],
   "source": [
    "print(\"Running Dmitry's Newton method on all problems!\\n\\n\")\n",
    "@printf \"%5s %30s %20s %20s %20s \\n\" \"i\" \"Problem name\" \"f(x)\" \"|grad(f(x))|\" \"its\"\n",
    "\n",
    "for i = 1:18\n",
    "    p = Problem(i)\n",
    "    \n",
    "    function obj(x)\n",
    "        return (p.obj(x), p.grd(x), p.hes(x))\n",
    "    end\n",
    "    \n",
    "    (x,f,gnorm,its) = newtonMethod(obj,p.x0)\n",
    "    @printf \"%5d %30s %20f %20f %20d \\n\" i p.name f gnorm its\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18                      Chebyquad             0.008949             1.018371                  500 \n",
      "Running Dmitry's BFGS method on all problems!\n",
      "\n",
      "    i                   Problem name                 f(x)         |grad(f(x))|                  its \n",
      "    1                Hellical valley             0.000004             0.046511                   24 \n",
      "    2                    Bigg's EXP6             0.005656             0.000124                   36 \n",
      "    3                       Gaussian             0.000000             0.000000                    6 \n",
      "    4                         Powell             0.000000             1.066644                   59 \n",
      "    5                      Box 3-dim             0.020431             0.010411                   17 \n",
      "    6           Variably dimensioned        425274.040251       9907956.334987                    8 \n",
      "    7                         Watson             0.000131             0.005266                   29 \n",
      "    8                      Penalty I           140.495195           164.944349                    9 \n",
      "    9                     Penalty II            88.360321            16.559610                   73 \n",
      "   10             Brown badly scaled             0.000000            25.161030                   12 \n",
      "   11                Brown and Denis         85824.779242           132.958948                   17 \n",
      "   12  Gulf research and development                  NaN                  NaN                   13 \n",
      "   13                  Trigonometric             0.000004             0.000004                   42 \n",
      "   14            Extended rosenbrock             0.001252             0.070794                  151 \n",
      "   15       Extended Powell singular             0.002348             0.173321                   31 \n",
      "   16                          Beale             0.000003             0.002182                   11 \n",
      "   17                           Wood             3.877164             1.624107                   18 \n",
      "   18                      Chebyquad             0.005386             0.000195                  131 \n"
     ]
    }
   ],
   "source": [
    "print(\"Running Dmitry's BFGS method on all problems!\\n\\n\")\n",
    "@printf \"%5s %30s %20s %20s %20s \\n\" \"i\" \"Problem name\" \"f(x)\" \"|grad(f(x))|\" \"its\"\n",
    "\n",
    "for i = 1:18\n",
    "    p = Problem(i)\n",
    "    \n",
    "    function obj(x)\n",
    "        return (p.obj(x), p.grd(x), p.hes(x))\n",
    "    end\n",
    "    \n",
    "    (x,f,gnorm,its) = bfgsMethod(obj,p.x0)\n",
    "    @printf \"%5d %30s %20f %20f %20d \\n\" i p.name f gnorm its\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
